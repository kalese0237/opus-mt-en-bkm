{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalese0237/opus-mt-en-bkm/blob/main/En_bkm_Parallel_Corpus_Creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-_baRZIwXB6"
      },
      "source": [
        "# **Converting excel data to dictionary format for huggingface dataset**\n",
        "\n",
        "The Dataset is made up of word and sentence pairs aligned in excel.\n",
        "\n",
        "The excel file is loaded onto google drive and imported to colab\n",
        "\n",
        "The imported dataset is then converted into a dictionary called \"Translation\". This dictionary is made up of two columns the first contains English words and sentences and the column title is \"aen\" (So that the English phrases appear first in the dictionary) The Kom words and phrases occupy the second columns and the columns head is bkm, the ISO code for Itangikom.\n",
        "\n",
        "The dataset is is made up of 3 split. The \"train\", \"test\", and \"validation\" splits. It is possible to have just one dataset and split in where neccessary.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVj-X8YwK2dF"
      },
      "source": [
        "The file is saved in a \".arrow\" format which is one of the file formats supported by HuggingFace datasets. The file is push to HuggingFace datasets https://huggingface.co/datasets/kalese\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHyCNxLSKqwW",
        "outputId": "70466c2c-2a15-4b3f-e22d-e1802a1d9d6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQoZ9s7Bn-sf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Step 1: Load the data from the Excel spreadsheet\n",
        "spreadsheet_path = \"/content/drive/MyDrive/Kom NMT Corpus/En-Bkm-NMT Complete Dataset.xlsx\"\n",
        "df = pd.read_excel(spreadsheet_path)\n",
        "\n",
        "# Step 2: Create a list to store the converted data\n",
        "hf_data = []\n",
        "\n",
        "# Step 3: Iterate over each row using iterrows and convert to the desired format\n",
        "for index, row in df.iterrows():\n",
        "    hf_data.append({\"aen\": row[\"aen\"], \"bkm\": row[\"bkm\"]})\n",
        "\n",
        "# Step 4: Create a Hugging Face dataset\n",
        "hf_dataset = Dataset.from_dict({\"translation\": hf_data})\n",
        "\n",
        "# Step 5: Save the dataset in a Hugging Face compatible format (optional)\n",
        "hf_dataset.save_to_disk(\"/content/drive/MyDrive/Kom NMT Corpus/hf_dataset1\")\n",
        "\n",
        "# Step 6: (Optional) Check the first few examples in the dataset\n",
        "print(hf_dataset[\"translation\"][:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sqF-wmuIoZO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6LeV7rySNaNz"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "# bkm = load_dataset('csv', data_files='data.txt')\n",
        "bkm = load_dataset('kalese/opus-mt-en-bkm')\n",
        "\n",
        "# Access the split you're interested in (e.g., 'train')\n",
        "dataset = bkm['train']\n",
        "\n",
        "# Split the data into training, test, and validation sets\n",
        "train_size = int(len(dataset) * 0.8)  # 80% of the data for training\n",
        "test_size = int(len(dataset) * 0.1)   # 10% of the data for testing\n",
        "val_size = int(len(dataset) * 0.1)    # Remaining data for validation\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, test_dataset, val_dataset = dataset.train_test_split(\n",
        "    test_size=test_size,\n",
        "    train_size=train_size,\n",
        "    val_size=val_size,\n",
        "    shuffle=True  # Shuffle the data before splitting\n",
        ")\n",
        "bkm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIMF3WL3KjDz"
      },
      "source": [
        "# **Inspecting the Dataset**\n",
        "The dataset is pulled from the HuggingFace Dataset repository and a summary of its features is displayed. We then display random entries of the various ssplits of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DsxDrdPLRbg",
        "outputId": "3a5121b0-bfcf-4ed5-ec71-7b9eac699972"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 20022\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['translation'],\n",
              "        num_rows: 2225\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "bkm = load_dataset('kalese/opus-mt-en-bkm')\n",
        "bkm = bkm[\"train\"].train_test_split(test_size=0.1)\n",
        "#bkm = bkm[\"train\"].train_val_split(val_size=0.1)\n",
        "\n",
        "bkm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXnoKIFrhqx3",
        "outputId": "0a30bbc3-994d-4ff8-9e77-ac9465671afe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'translation': {'aen': 'Living Out the Message Understand this, my dear brothers and sisters! Let every person be quick to listen, slow to speak, slow to anger.',\n",
              "  'bkm': \" Yì to' yvɨ̀tɨ iyeynì a woyn-nà ghem ghɨ jûŋ: Ghɨ se sɨ bè ɨ be iwo nô mɨ nda nà lêm àtu sɨ nà tô' yvɨtɨ-à, yi kfeynɨ na wù taŋi, wu tò' ɨ kfà'tɨ̀ jæ na wù taŋi. Ka wùl na shɨŋ nyô'sɨ̀ nyo'sɨ ɨtoŋ iwo.\"}}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bkm['test'][400]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnCSghDWoaJg",
        "outputId": "417a1b57-f9ba-44a4-c9f1-2439a5629554"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'translation': {'aen': 'Just as he finished speaking, the king’s sons arrived, wailing and weeping. The king and all his servants wept loudly as well.',\n",
              "  'bkm': ' Wù nà n-taŋî mesì gvî kɨ tèyn woyn ntoʼ ghì ɨ zɨ nɨ̀ ìdzɨ-ì kɨ ghɨ̀jɨ̀m. Fòyn ŋêyn ghelɨ ghɨ felɨnɨ nɨ̀ ŋweyn ɨ boŋ fe kɨ nɨ̀ idzɨ-ì.'}}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bkm[\"train\"][400]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4Mz_-IXpzjb"
      },
      "outputs": [],
      "source": [
        "bkm[\"train\"][8]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omT64Zszqrv2"
      },
      "outputs": [],
      "source": [
        "bkm[\"validation\"][3000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNQU60y1Q8fB"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "# Step 1: Load the data from the Excel spreadsheet\n",
        "spreadsheet_path = \"/content/drive/MyDrive/English - Kom Lexicon 1.xlsx\"\n",
        "df = pd.read_excel(spreadsheet_path)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUZiNXJcRXbL"
      },
      "outputs": [],
      "source": [
        "# Step 2: Create a list to store the converted data\n",
        "hf_data = []\n",
        "\n",
        "# Step 3: Iterate over each row using iterrows and convert to the desired format\n",
        "for index, row in df.iterrows():\n",
        "    hf_data.append({\"aen\": row[\"aen\"], \"bkm\": row[\"bkm\"]})\n",
        "\n",
        "# Step 4: Create a Hugging Face dataset\n",
        "hf_dataset = Dataset.from_dict({\"translation\": hf_data})\n",
        "\n",
        "# Step 5: Save the dataset in a Hugging Face compatible format (optional)\n",
        "hf_dataset.save_to_disk(\"/content/drive/MyDrive/Kom NMT Corpus/hf_dataset1\")\n",
        "\n",
        "# Step 6: (Optional) Check the first few examples in the dataset\n",
        "print(hf_dataset[\"translation\"][:5])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1nAw3cltv9t98eHUqy1OmS0Gs1z2YJRO1",
      "authorship_tag": "ABX9TyOHHOEVVCvljGjgQ5jB+5j3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}